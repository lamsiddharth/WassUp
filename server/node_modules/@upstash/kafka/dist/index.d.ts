type Request = {
    path: string[];
    /**
     * Request body will be serialized to json
     */
    body?: unknown;
    headers?: Record<string, string>;
    retries?: number;
};
type HttpClientConfig = {
    headers?: Record<string, string>;
    baseUrl: string;
};
declare class HttpClient {
    readonly baseUrl: string;
    readonly headers: Record<string, string>;
    constructor(config: HttpClientConfig);
    private request;
    get<TResponse>(req: Request): Promise<TResponse>;
    post<TResponse>(req: Request): Promise<TResponse>;
}

/**
 * Optional parameters for each produced message
 */
type ProduceOptions = {
    /**
     * The partition to produce to.
     * Will be assigned by kafka if left empty.
     */
    partition?: number;
    /**
     * The unix timestamp in seconds.
     * Will be assigned by kafka if left empty.
     */
    timestamp?: number;
    /**
     * Events with the same event key (e.g., a customer or vehicle ID) are written
     * to the same partition, and Kafka guarantees that any consumer of a given
     * topic-partition will always read that partition's events in exactly the
     * same order as they were written.
     */
    key?: string;
    headers?: {
        key: string;
        value: string;
    }[];
};
/**
 * Request payload to produce a message to a topic.
 */
type ProduceRequest = ProduceOptions & {
    /**
     * The topic where the message gets publish.
     * Make sure this exists in upstash before. Otherwise it will throw an error.
     */
    topic: string;
    /**
     * The message itself. This will be serialized using `JSON.stringify`
     */
    value: unknown;
};
/**
 * Response for each successfull message produced
 */
type ProduceResponse = {
    topic: string;
    partition: number;
    offset: number;
    timestamp: number;
};
declare class Producer {
    private readonly client;
    constructor(client: HttpClient);
    /**
     * Produce a single message to a single topic
     */
    produce<TMessage>(topic: string, message: TMessage, opts?: ProduceOptions): Promise<ProduceResponse>;
    /**
     * Produce multiple messages to different topics at the same time
     *
     * Each entry in the response array belongs to the request with the same order in the requests.
     */
    produceMany(requests: ProduceRequest[]): Promise<ProduceResponse[]>;
}

type Header = {
    key: string;
    value: string;
};
type Message = {
    topic: string;
    partition: number;
    offset: number;
    timestamp: number;
    key: string;
    value: string;
    headers: Header[];
};

type TopicPartition = {
    topic: string;
    partition: number;
};
type TopicPartitionOffset = TopicPartition & {
    offset: number;
};
type FetchRequest = {
    timeout?: number;
    topicPartitionOffsets?: TopicPartitionOffset[];
} & ({
    topic: string;
    partition: number;
    offset: number;
} | {
    topic?: never;
    partition?: never;
    offset?: never;
});
type BaseConsumerRequest = {
    /**
     * The name of the consumer group which is used as Kafka consumer group id
     * @see https://kafka.apache.org/documentation/#consumerconfigs_group.id
     */
    consumerGroupId: string;
    /**
     * Used to identify kafka consumer instances in the same consumer group.
     * Each consumer instance id is handled by a separate consumer client.
     * @see https://kafka.apache.org/documentation/#consumerconfigs_group.instance.id
     */
    instanceId: string;
};
type ConsumeRequest = BaseConsumerRequest & {
    topics: string[];
    /**
     * Defines the time to wait at most for the fetch request in milliseconds.
     * It's optional and its default value 1000.
     */
    timeout?: number;
    /**
     * If true, the consumer's offset will be periodically committed in the background.
     */
    autoCommit?: boolean;
    /**
     * The frequency in milliseconds that the consumer offsets are auto-committed to Kafka
     * if auto commit is enabled.
     * Default is 5000.
     */
    autoCommitInterval?: number;
    /**
     * What to do when there is no initial offset in Kafka or if the current
     * offset does not exist any more on the server. Default value is `latest`.
     *
     * `earliest`: Automatically reset the offset to the earliest offset
     *
     * `latest`: Automatically reset the offset to the latest offset
     *
     * `none`: Throw exception to the consumer if no previous offset is found for the
     * consumer's group.
     */
    autoOffsetReset?: "earliest" | "latest" | "none";
};
type FetchOptions = {
    /**
     * If true `fetch` will call upstash once for each topic in your request.
     * This circumenvents the issue where upstash only returns from a single topic
     * at a time when using fetch.
     *
     * All requests are executed in parallel.
     *
     * Default: true
     */
    parallel?: boolean;
};
type CommitRequest = BaseConsumerRequest & {
    /**
     * Commits the last consumed messages if left empty
     */
    offset?: TopicPartitionOffset | TopicPartitionOffset[];
};
type CommittedRequest = BaseConsumerRequest & {
    topicPartitions: TopicPartition[];
};
/**
 * Consumer APIs are used to fetch/consume messages from Kafka topics. Similar
 * to Kafka clients there are two mechanisms to consume messages; one is
 * seeking offsets manually and the other is to use consumer groups which
 * manage offsets automatically inside a special Kafka topic.
 *
 * We call the first one as Fetch API and the second one as Consume API.
 * Consume API has some additional methods if you wish to commit offsets manually.
 */
declare class Consumer {
    private readonly client;
    constructor(client: HttpClient);
    /**
     * Fetches the message(s) starting with a given offset inside the partition.
     * This API doesn't use consumer groups.
     *
     * When fetching from multiple topics it is important to understand that
     * upstash only returns data for a single topic at a time, so you should
     * call `fetch` multiple times.
     *
     * Fetch from a single <topic, partition, offset>:
     * ```ts
     *   fetch({
     *        topic: "greeting",
     *        partition: 3,
     *        timeout: 1000
     *    })
     * ```
     *
     * Fetch from multiple <topic, partition, offset> triples:
     * ```ts
     *    fetch({
     *       topicPartitionOffsets": [
     *            {"topic": "greetings", "partition": 1, "offset": 1},
     *            {"topic": "greetings", "partition": 2, "offset": 1},
     *            {"topic": "greetings", "partition": 3, "offset": 1},
     *            {"topic": "cities", "partition": 1, "offset": 10},
     *            {"topic": "cities", "partition": 2, "offset": 20}
     *        ],
     *        timeout: 1000
     *    })
     * ```
     *
     * You can even combine both:
     * ```ts
     *    fetch({
     *        topic: "words",
     *        partition: 0,
     *        offset: 0,
     *        topicPartitionOffsets: [
     *            { topic: "cities", partition: 1, offset: 10},
     *            { topic: "cities", partition: 2, offset: 20}
     *        ],
     *        timeout: 5000
     *    })
     * ```
     */
    fetch(req: FetchRequest, opts?: FetchOptions): Promise<Message[]>;
    /**
     * Fetches the message(s) using Kafka consumer group mechanism and may commit
     * the offsets automatically.
     *
     * The first time a consumer is created, it needs to figure out the group
     * coordinator by asking the Kafka brokers and joins the consumer group.
     * This process takes some time to complete. That's why when a consumer
     * instance is created first time, it may return empty messages until consumer
     * group coordination is completed.
     *
     * Consume from a single topic with timeout:
     * ```ts
     *  consume({
     *    consumerGroupId: "mygroup",
     *    instanceId: "myconsumer",
     *    topics: ["greetings"]
     *  })
     * ```
     *
     * Consume from multiple topics:
     * ```ts
     *  consume({
     *    consumerGroupId: "mygroup",
     *    instanceId: "myconsumer",
     *    topics: ["greetings", "cities", "world"],
     *    timeout: 1000
     *  })
     * ```
     *
     *  Consume from topics without auto commit:
     * ```ts
     *  consume({
     *    consumerGroupId: "mygroup",
     *    instanceId: "myconsumer",
     *    topics: ["greetings", "cities", "world"],
     *    timeout: 1000,
     *    autoCommit: false
     *  })
     * ```
     *
     *  Consume from topics starting from the earliest message:
     * ```ts
     *  consume({
     *    consumerGroupId: "mygroup",
     *    instanceId: "myconsumer",
     *    topics: ["greetings", "cities", "world"],
     *    timeout: 1000,
     *    autoOffsetReset: "earliest"
     *  })
     * ```
     *
     *  Consume from topics with custom auto commit interval:
     * ```ts
     *  consume({
     *    consumerGroupId: "mygroup",
     *    instanceId: "myconsumer",
     *    topics: ["greetings", "cities", "world"],
     *    timeout: 1000,
     *    autoCommit: true,
     *    autoCommitInterval: 3000
     *  })
     * ```
     */
    consume(req: ConsumeRequest): Promise<Message[]>;
    /**
     * Commits the fetched message offsets. `commit` should be used alongside
     * `consume`, especially when auto commit is disabled.
     *
     *  Commit single topic partition offset:
     *  ```ts
     *    commit({
     *      consumerGroupId: "mygroup",
     *      instanceId: "myconsumer",
     *      offset: {
     *        topic: "cities",
     *        partition: 1,
     *        offset: 10,
     *      }
     *    })
     *  ```
     *
     *  Commit multiple topic partition offsets:
     *  ```ts
     *    commit({
     *      consumerGroupId: "mygroup",
     *      instanceId: "myconsumer",
     *      offset: [
     *        { topic: "cities", partition: 0, offset: 13 },
     *        { topic: "cities", partition: 1, offset: 37 },
     *        { topic: "greetings", partition: 0, offset: 19 },
     *      ]
     *    })
     *  ```
     *
     *  Commit all latest consumed message offsets:
     *  ```ts
     *    commit({
     *      consumerGroupId: "mygroup",
     *      instanceId: "myconsumer",
     *    })
     *  ```
     */
    commit(req: CommitRequest): Promise<void>;
    /**
     *  Returns the last committed offsets for the topic partitions inside the group.
     *
     * List committed offsets for multiple topic partitions:
     *  ```ts
     *    committed({
     *      consumerGroupId: "mygroup",
     *      instanceId: "myconsumer",
     *      topicPartitions: [
     *        { topic: "cities", partition: 0 },
     *        { topic: "cities", partition: 1 },
     *        { topic: "greetings", partition: 0},
     *      ]
     *    })
     *  ```
     */
    committed(req: CommittedRequest): Promise<TopicPartitionOffset[]>;
}

type OffsetsRequest = {
    consumerGroupId: string;
    instanceId: string;
} & ({
    topicPartition?: never;
    topicPartitions: TopicPartition[];
} | {
    topicPartition: TopicPartition;
    topicPartitions?: never;
});
type TopicPartitionOffsetsRequest = {
    /**
     * Unix timestamp in milliseconds or `earliest` or `latest`
     */
    timestamp: number | "earliest" | "latest";
} & ({
    topicPartition?: never;
    topicPartitions: TopicPartition[];
} | {
    topicPartition: TopicPartition;
    topicPartitions?: never;
});
/**
 * Topic names and their partitions
 */
type GetTopicsResponse = {
    [topic: string]: number;
};
type TopicAssignments = {
    topic: string;
    partitions: number[];
};
type InstanceAssignments = {
    name: string;
    topics: TopicAssignments[];
};
type GroupAssignments = {
    name: string;
    instances: InstanceAssignments[];
};
declare class Admin {
    private readonly client;
    constructor(client: HttpClient);
    /**
     * List all topics belonging to the user
     */
    topics(): Promise<GetTopicsResponse>;
    /**
     * Lists consumers belonging to the user known by the REST server.
     */
    consumers(): Promise<GroupAssignments[]>;
    /**
     * Stops and removes a previously created consumer group instance.
     */
    removeConsumerInstance(consumerGroup: string, instanceId: string): Promise<void>;
    /**
     * Returns the last committed offsets for the topic partitions inside the group. Can be used
     * alongside Commit Consumer API.
     */
    committedOffsets(req: OffsetsRequest): Promise<TopicPartitionOffset[]>;
    /**
     * Returns the offsets for the given partitions by timestamp. The returned offset for each
     * partition is the earliest offset whose timestamp is greater than or equal to the given
     * timestamp in the corresponding partition.
     */
    topicPartitionOffsets(req: TopicPartitionOffsetsRequest): Promise<TopicPartitionOffset[]>;
}

/**
 * Connection credentials for upstash kafka.
 * Get them from https://console.upstash.com/kafka/<uuid>
 */
type KafkaConfig = {
    /**
     * UPSTASH_KAFKA_REST_URL
     */
    url: string;
    /**
     * UPSTASH_KAFKA_REST_USERNAME
     */
    username: string;
    /**
     * UPSTASH_KAFKA_REST_PASSWORD
     */
    password: string;
};
/**
 * Serverless Kafka client for upstash.
 */
declare class Kafka {
    private readonly client;
    /**
     * Create a new kafka client
     *
     * @example
     * ```typescript
     * const kafka = new Kafka({
     *  url: "<UPSTASH_KAFKA_REST_URL>",
     *  username: "<UPSTASH_KAFKA_REST_USERNAME>",
     *  password: "<UPSTASH_KAFKA_REST_PASSWORD>",
     * });
     * ```
     */
    constructor(config: KafkaConfig);
    /**
     * Create a new producer client
     */
    producer(): Producer;
    /**
     * Create a new consumer client
     */
    consumer(): Consumer;
    /**
     * Create a new admin client
     */
    admin(): Admin;
}

/**
 * Result of a bad request to upstash
 */
declare class UpstashError extends Error {
    readonly result: string;
    readonly error: string;
    readonly status: number;
    constructor(res: {
        result: string;
        error: string;
        status: number;
    });
}

export { Consumer, Header, Kafka, KafkaConfig, Message, Producer, UpstashError };
